// THIS FILE IS AUTO-GENERATED, DO NOT EDIT. Changes should be made to package.scala.in

package com.microsoft.scalatorch.torch.nn

import com.microsoft.scalatorch.torch
import com.microsoft.scalatorch.torch._
import com.microsoft.scalatorch.torch.util.Implicits._
import com.microsoft.scalatorch.torch.internal.{ TensorIndex, TensorVector, TorchTensor, LongVector, torch_swig => swig }
import com.microsoft.scalatorch.torch.util.NoGrad

package object functional {
  // This overload is only here because the defaults are missing from the auto-generated version.
  // Shouldn't be needed once a version of libtorch with https://github.com/pytorch/pytorch/pull/70156 is released.
  def poisson_nll_loss(
      input: Tensor,
      target: Tensor,
      log_input: Boolean = false,
      full: Boolean = false,
      eps: Double = 1e-8,
      reduction: Reduction = Reduction.Mean,
  )(implicit cg: ReferenceManager): Tensor = {
    torch.poisson_nll_loss(
      input,
      target,
      log_input,
      full,
      eps,
      reduction = reduction.swigValue(),
    )
  }

// THIS FILE IS AUTOMATICALLY GENERATED, DO NOT EDIT
// See swig/src/main/swig/build.sbt for details
  def binary_cross_entropy(self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.binary_cross_entropy(self.underlying, target.underlying, weight.map(_.underlying), reduction))
  def binary_cross_entropy_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.binary_cross_entropy_backward(grad_output.underlying, self.underlying, target.underlying, weight.map(_.underlying), reduction))
  def linear(input: Tensor, weight: Tensor, bias: Option[Tensor] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.linear(input.underlying, weight.underlying, bias.map(_.underlying)))
  def mkldnn_linear(self: Tensor, weight: Tensor, bias: Option[Tensor] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mkldnn_linear(self.underlying, weight.underlying, bias.map(_.underlying)))
  def relu6(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.relu6(self.underlying))
  def relu6_(self: Tensor)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.relu6_(self.underlying)
    self
  }
  def gelu(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.gelu(self.underlying))
  def gelu_backward(grad: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.gelu_backward(grad.underlying, self.underlying))
  def infinitely_differentiable_gelu_backward(grad: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.infinitely_differentiable_gelu_backward(grad.underlying, self.underlying))
  def silu(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.silu(self.underlying))
  def silu_(self: Tensor)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.silu_(self.underlying)
    self
  }
  def silu_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.silu_backward(grad_output.underlying, self.underlying))
  def mish(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mish(self.underlying))
  def mish_(self: Tensor)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.mish_(self.underlying)
    self
  }
  def mish_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mish_backward(grad_output.underlying, self.underlying))
  def one_hot(self: Tensor, num_classes: Long = -1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.one_hot(self.underlying, num_classes))
  def mkldnn_reorder_conv2d_weight(self: Tensor, padding: Array[Long] = Array(0), stride: Array[Long] = Array(1), dilation: Array[Long] = Array(1), groups: Long = 1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mkldnn_reorder_conv2d_weight(self.underlying, padding, stride, dilation, groups))
  def mkldnn_reorder_conv3d_weight(self: Tensor, padding: Array[Long] = Array(0), stride: Array[Long] = Array(1), dilation: Array[Long] = Array(1), groups: Long = 1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mkldnn_reorder_conv3d_weight(self.underlying, padding, stride, dilation, groups))
  def cross_entropy_loss(self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue(), ignore_index: Long = -100, label_smoothing: Double = 0.0)(implicit rm: ReferenceManager): Tensor = Tensor(swig.cross_entropy_loss(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index, label_smoothing))
  def mse_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.mse_loss(self.underlying, target.underlying, reduction))
  def mse_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long)(implicit rm: ReferenceManager): Tensor = Tensor(swig.mse_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction))
  def l1_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.l1_loss(self.underlying, target.underlying, reduction))
  def l1_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long)(implicit rm: ReferenceManager): Tensor = Tensor(swig.l1_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction))
  def multi_margin_loss(self: Tensor, target: Tensor, p: Double = 1, margin: Double = 1, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.multi_margin_loss(self.underlying, target.underlying, p.toInternalScalar, margin.toInternalScalar, weight.map(_.underlying), reduction))
  def multi_margin_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, p: Scalar, margin: Scalar, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.multi_margin_loss_backward(grad_output.underlying, self.underlying, target.underlying, p.underlying, margin.underlying, weight.map(_.underlying), reduction))
  def multilabel_margin_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.multilabel_margin_loss(self.underlying, target.underlying, reduction))
  def multilabel_margin_loss_forward(self: Tensor, target: Tensor, reduction: Long)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.multilabel_margin_loss_forward(self.underlying, target.underlying, reduction))
  def multilabel_margin_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long, is_target: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.multilabel_margin_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction, is_target.underlying))
  def nll_loss_nd(self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue(), ignore_index: Long = -100)(implicit rm: ReferenceManager): Tensor = Tensor(swig.nll_loss_nd(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index))
  def nll_loss(self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue(), ignore_index: Long = -100)(implicit rm: ReferenceManager): Tensor = Tensor(swig.nll_loss(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index))
  def nll_loss_forward(self: Tensor, target: Tensor, weight: Option[Tensor], reduction: Long, ignore_index: Long)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.nll_loss_forward(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index))
  def nll_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Option[Tensor], reduction: Long, ignore_index: Long, total_weight: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.nll_loss_backward(grad_output.underlying, self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index, total_weight.underlying))
  def nll_loss2d(self: Tensor, target: Tensor, weight: Option[Tensor] = None, reduction: Long = internal.Reduction.Mean.swigValue(), ignore_index: Long = -100)(implicit rm: ReferenceManager): Tensor = Tensor(swig.nll_loss2d(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index))
  def nll_loss2d_forward(self: Tensor, target: Tensor, weight: Option[Tensor], reduction: Long, ignore_index: Long)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.nll_loss2d_forward(self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index))
  def nll_loss2d_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Option[Tensor], reduction: Long, ignore_index: Long, total_weight: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.nll_loss2d_backward(grad_output.underlying, self.underlying, target.underlying, weight.map(_.underlying), reduction, ignore_index, total_weight.underlying))
  def smooth_l1_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue(), beta: Double = 1.0)(implicit rm: ReferenceManager): Tensor = Tensor(swig.smooth_l1_loss(self.underlying, target.underlying, reduction, beta))
  def smooth_l1_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long, beta: Double)(implicit rm: ReferenceManager): Tensor = Tensor(swig.smooth_l1_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction, beta))
  def huber_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue(), delta: Double = 1.0)(implicit rm: ReferenceManager): Tensor = Tensor(swig.huber_loss(self.underlying, target.underlying, reduction, delta))
  def huber_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long, delta: Double)(implicit rm: ReferenceManager): Tensor = Tensor(swig.huber_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction, delta))
  def soft_margin_loss(self: Tensor, target: Tensor, reduction: Long = internal.Reduction.Mean.swigValue())(implicit rm: ReferenceManager): Tensor = Tensor(swig.soft_margin_loss(self.underlying, target.underlying, reduction))
  def soft_margin_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: Long)(implicit rm: ReferenceManager): Tensor = Tensor(swig.soft_margin_loss_backward(grad_output.underlying, self.underlying, target.underlying, reduction))
  def elu(self: Tensor, alpha: Double = 1, scale: Double = 1, input_scale: Double = 1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.elu(self.underlying, alpha.toInternalScalar, scale.toInternalScalar, input_scale.toInternalScalar))
  def elu_backward(grad_output: Tensor, alpha: Scalar, scale: Scalar, input_scale: Scalar, is_result: Boolean, self_or_result: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.elu_backward(grad_output.underlying, alpha.underlying, scale.underlying, input_scale.underlying, is_result, self_or_result.underlying))
  def elu_(self: Tensor, alpha: Double = 1, scale: Double = 1, input_scale: Double = 1)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.elu_(self.underlying, alpha.toInternalScalar, scale.toInternalScalar, input_scale.toInternalScalar)
    self
  }
  def glu(self: Tensor, dim: Long = -1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.glu(self.underlying, dim))
  def glu_backward(grad_output: Tensor, self: Tensor, dim: Long)(implicit rm: ReferenceManager): Tensor = Tensor(swig.glu_backward(grad_output.underlying, self.underlying, dim))
  def hardsigmoid(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardsigmoid(self.underlying))
  def hardsigmoid_(self: Tensor)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.hardsigmoid_(self.underlying)
    self
  }
  def hardsigmoid_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardsigmoid_backward(grad_output.underlying, self.underlying))
  def hardtanh(self: Tensor, min_val: Double = -1, max_val: Double = 1)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardtanh(self.underlying, min_val.toInternalScalar, max_val.toInternalScalar))
  def hardtanh_backward(grad_output: Tensor, self: Tensor, min_val: Scalar, max_val: Scalar)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardtanh_backward(grad_output.underlying, self.underlying, min_val.underlying, max_val.underlying))
  def hardtanh_(self: Tensor, min_val: Double = -1, max_val: Double = 1)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.hardtanh_(self.underlying, min_val.toInternalScalar, max_val.toInternalScalar)
    self
  }
  def hardswish(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardswish(self.underlying))
  def hardswish_(self: Tensor)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.hardswish_(self.underlying)
    self
  }
  def hardswish_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.hardswish_backward(grad_output.underlying, self.underlying))
  def leaky_relu(self: Tensor, negative_slope: Double = 0.01)(implicit rm: ReferenceManager): Tensor = Tensor(swig.leaky_relu(self.underlying, negative_slope.toInternalScalar))
  def leaky_relu_backward(grad_output: Tensor, self: Tensor, negative_slope: Scalar, self_is_result: Boolean)(implicit rm: ReferenceManager): Tensor = Tensor(swig.leaky_relu_backward(grad_output.underlying, self.underlying, negative_slope.underlying, self_is_result))
  def leaky_relu_(self: Tensor, negative_slope: Double = 0.01)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.leaky_relu_(self.underlying, negative_slope.toInternalScalar)
    self
  }
  def log_sigmoid(self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.log_sigmoid(self.underlying))
  def log_sigmoid_forward(self: Tensor)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.log_sigmoid_forward(self.underlying))
  def log_sigmoid_backward(grad_output: Tensor, self: Tensor, buffer: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.log_sigmoid_backward(grad_output.underlying, self.underlying, buffer.underlying))
  def rrelu_with_noise(self: Tensor, noise: Tensor, lower: Double = 0.125, upper: Double = 0.3333333333333333, training: Boolean = false, generator: Option[Generator] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.rrelu_with_noise(self.underlying, noise.underlying, lower.toInternalScalar, upper.toInternalScalar, training, generator.map(_.underlying)))
  def rrelu_with_noise_backward(grad_output: Tensor, self: Tensor, noise: Tensor, lower: Scalar, upper: Scalar, training: Boolean, self_is_result: Boolean)(implicit rm: ReferenceManager): Tensor = Tensor(swig.rrelu_with_noise_backward(grad_output.underlying, self.underlying, noise.underlying, lower.underlying, upper.underlying, training, self_is_result))
  def rrelu_with_noise_(self: Tensor, noise: Tensor, lower: Double = 0.125, upper: Double = 0.3333333333333333, training: Boolean = false, generator: Option[Generator] = None)(implicit rm: ReferenceManager): self.type = NoGrad.noGrad {
    swig.rrelu_with_noise_(self.underlying, noise.underlying, lower.toInternalScalar, upper.toInternalScalar, training, generator.map(_.underlying))
    self
  }
  def softplus(self: Tensor, beta: Double = 1, threshold: Double = 20)(implicit rm: ReferenceManager): Tensor = Tensor(swig.softplus(self.underlying, beta.toInternalScalar, threshold.toInternalScalar))
  def softplus_backward(grad_output: Tensor, self: Tensor, beta: Scalar, threshold: Scalar, output: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.softplus_backward(grad_output.underlying, self.underlying, beta.underlying, threshold.underlying, output.underlying))
  def softshrink(self: Tensor, lambd: Double = 0.5)(implicit rm: ReferenceManager): Tensor = Tensor(swig.softshrink(self.underlying, lambd.toInternalScalar))
  def softshrink_backward(grad_output: Tensor, self: Tensor, lambd: Scalar)(implicit rm: ReferenceManager): Tensor = Tensor(swig.softshrink_backward(grad_output.underlying, self.underlying, lambd.underlying))
  def adaptive_avg_pool2d(self: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.adaptive_avg_pool2d(self.underlying, output_size))
  def _adaptive_avg_pool2d_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig._adaptive_avg_pool2d_backward(grad_output.underlying, self.underlying))
  def adaptive_avg_pool3d(self: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.adaptive_avg_pool3d(self.underlying, output_size))
  def _adaptive_avg_pool3d_backward(grad_output: Tensor, self: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig._adaptive_avg_pool3d_backward(grad_output.underlying, self.underlying))
  def adaptive_max_pool2d(self: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.adaptive_max_pool2d(self.underlying, output_size))
  def adaptive_max_pool2d_backward(grad_output: Tensor, self: Tensor, indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.adaptive_max_pool2d_backward(grad_output.underlying, self.underlying, indices.underlying))
  def adaptive_max_pool3d(self: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.adaptive_max_pool3d(self.underlying, output_size))
  def adaptive_max_pool3d_backward(grad_output: Tensor, self: Tensor, indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.adaptive_max_pool3d_backward(grad_output.underlying, self.underlying, indices.underlying))
  def avg_pool2d(self: Tensor, kernel_size: Array[Long], stride: Array[Long] = Array(), padding: Array[Long] = Array(0), ceil_mode: Boolean = false, count_include_pad: Boolean = true, divisor_override: Option[Long] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.avg_pool2d(self.underlying, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.asJavaLong))
  def avg_pool2d_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], ceil_mode: Boolean, count_include_pad: Boolean, divisor_override: Option[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.avg_pool2d_backward(grad_output.underlying, self.underlying, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.asJavaLong))
  def avg_pool3d(self: Tensor, kernel_size: Array[Long], stride: Array[Long] = Array(), padding: Array[Long] = Array(0), ceil_mode: Boolean = false, count_include_pad: Boolean = true, divisor_override: Option[Long] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.avg_pool3d(self.underlying, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.asJavaLong))
  def avg_pool3d_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], ceil_mode: Boolean, count_include_pad: Boolean, divisor_override: Option[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.avg_pool3d_backward(grad_output.underlying, self.underlying, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.asJavaLong))
  def fractional_max_pool2d(self: Tensor, kernel_size: Array[Long], output_size: Array[Long], random_samples: Tensor)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.fractional_max_pool2d(self.underlying, kernel_size, output_size, random_samples.underlying))
  def fractional_max_pool2d_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], output_size: Array[Long], indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.fractional_max_pool2d_backward(grad_output.underlying, self.underlying, kernel_size, output_size, indices.underlying))
  def fractional_max_pool3d(self: Tensor, kernel_size: Array[Long], output_size: Array[Long], random_samples: Tensor)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.fractional_max_pool3d(self.underlying, kernel_size, output_size, random_samples.underlying))
  def fractional_max_pool3d_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], output_size: Array[Long], indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.fractional_max_pool3d_backward(grad_output.underlying, self.underlying, kernel_size, output_size, indices.underlying))
  def max_pool2d_with_indices(self: Tensor, kernel_size: Array[Long], stride: Array[Long] = Array(), padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1), ceil_mode: Boolean = false)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.max_pool2d_with_indices(self.underlying, kernel_size, stride, padding, dilation, ceil_mode))
  def max_pool2d_with_indices_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], ceil_mode: Boolean, indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_pool2d_with_indices_backward(grad_output.underlying, self.underlying, kernel_size, stride, padding, dilation, ceil_mode, indices.underlying))
  def max_pool3d_with_indices(self: Tensor, kernel_size: Array[Long], stride: Array[Long] = Array(), padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1), ceil_mode: Boolean = false)(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig.max_pool3d_with_indices(self.underlying, kernel_size, stride, padding, dilation, ceil_mode))
  def max_pool3d_with_indices_backward(grad_output: Tensor, self: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], ceil_mode: Boolean, indices: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_pool3d_with_indices_backward(grad_output.underlying, self.underlying, kernel_size, stride, padding, dilation, ceil_mode, indices.underlying))
  def max_unpool2d(self: Tensor, indices: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_unpool2d(self.underlying, indices.underlying, output_size))
  def max_unpool2d_backward(grad_output: Tensor, self: Tensor, indices: Tensor, output_size: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_unpool2d_backward(grad_output.underlying, self.underlying, indices.underlying, output_size))
  def max_unpool3d(self: Tensor, indices: Tensor, output_size: Array[Long], stride: Array[Long], padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_unpool3d(self.underlying, indices.underlying, output_size, stride, padding))
  def max_unpool3d_backward(grad_output: Tensor, self: Tensor, indices: Tensor, output_size: Array[Long], stride: Array[Long], padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.max_unpool3d_backward(grad_output.underlying, self.underlying, indices.underlying, output_size, stride, padding))
  def reflection_pad1d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad1d(self.underlying, padding))
  def reflection_pad1d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad1d_backward(grad_output.underlying, self.underlying, padding))
  def reflection_pad2d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad2d(self.underlying, padding))
  def reflection_pad2d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad2d_backward(grad_output.underlying, self.underlying, padding))
  def reflection_pad3d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad3d(self.underlying, padding))
  def reflection_pad3d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.reflection_pad3d_backward(grad_output.underlying, self.underlying, padding))
  def replication_pad1d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad1d(self.underlying, padding))
  def replication_pad1d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad1d_backward(grad_output.underlying, self.underlying, padding))
  def replication_pad2d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad2d(self.underlying, padding))
  def replication_pad2d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad2d_backward(grad_output.underlying, self.underlying, padding))
  def replication_pad3d(self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad3d(self.underlying, padding))
  def replication_pad3d_backward(grad_output: Tensor, self: Tensor, padding: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.replication_pad3d_backward(grad_output.underlying, self.underlying, padding))
  def upsample_linear1d(input: Tensor, output_size: Option[Array[Long]], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_linear1d(input.underlying, output_size, align_corners, scale_factors))
  def upsample_linear1d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_linear1d_backward(grad_output.underlying, output_size, input_size, align_corners, scale_factors))
  def upsample_bilinear2d(input: Tensor, output_size: Option[Array[Long]], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bilinear2d(input.underlying, output_size, align_corners, scale_factors))
  def upsample_bilinear2d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bilinear2d_backward(grad_output.underlying, output_size, input_size, align_corners, scale_factors))
  def upsample_trilinear3d(input: Tensor, output_size: Option[Array[Long]], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_trilinear3d(input.underlying, output_size, align_corners, scale_factors))
  def upsample_trilinear3d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_trilinear3d_backward(grad_output.underlying, output_size, input_size, align_corners, scale_factors))
  def upsample_bicubic2d(input: Tensor, output_size: Option[Array[Long]], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bicubic2d(input.underlying, output_size, align_corners, scale_factors))
  def upsample_bicubic2d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], align_corners: Boolean, scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bicubic2d_backward(grad_output.underlying, output_size, input_size, align_corners, scale_factors))
  def upsample_nearest1d(input: Tensor, output_size: Option[Array[Long]], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest1d(input.underlying, output_size, scale_factors))
  def upsample_nearest1d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest1d_backward(grad_output.underlying, output_size, input_size, scale_factors))
  def upsample_nearest2d(input: Tensor, output_size: Option[Array[Long]], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest2d(input.underlying, output_size, scale_factors))
  def upsample_nearest2d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest2d_backward(grad_output.underlying, output_size, input_size, scale_factors))
  def upsample_nearest3d(input: Tensor, output_size: Option[Array[Long]], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest3d(input.underlying, output_size, scale_factors))
  def upsample_nearest3d_backward(grad_output: Tensor, output_size: Option[Array[Long]], input_size: Array[Long], scale_factors: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest3d_backward(grad_output.underlying, output_size, input_size, scale_factors))
  def upsample_linear1d(self: Tensor, output_size: Array[Long], align_corners: Boolean, scales: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_linear1d(self.underlying, output_size, align_corners, scales.asJavaDouble))
  def upsample_linear1d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], align_corners: Boolean, scales: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_linear1d_backward(grad_output.underlying, output_size, input_size, align_corners, scales.asJavaDouble))
  def upsample_bilinear2d(self: Tensor, output_size: Array[Long], align_corners: Boolean, scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bilinear2d(self.underlying, output_size, align_corners, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_bilinear2d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], align_corners: Boolean, scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bilinear2d_backward(grad_output.underlying, output_size, input_size, align_corners, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_bicubic2d(self: Tensor, output_size: Array[Long], align_corners: Boolean, scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bicubic2d(self.underlying, output_size, align_corners, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_bicubic2d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], align_corners: Boolean, scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_bicubic2d_backward(grad_output.underlying, output_size, input_size, align_corners, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_trilinear3d(self: Tensor, output_size: Array[Long], align_corners: Boolean, scales_d: Option[Double], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_trilinear3d(self.underlying, output_size, align_corners, scales_d.asJavaDouble, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_trilinear3d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], align_corners: Boolean, scales_d: Option[Double], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_trilinear3d_backward(grad_output.underlying, output_size, input_size, align_corners, scales_d.asJavaDouble, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_nearest1d(self: Tensor, output_size: Array[Long], scales: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest1d(self.underlying, output_size, scales.asJavaDouble))
  def upsample_nearest1d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], scales: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest1d_backward(grad_output.underlying, output_size, input_size, scales.asJavaDouble))
  def upsample_nearest2d(self: Tensor, output_size: Array[Long], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest2d(self.underlying, output_size, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_nearest2d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest2d_backward(grad_output.underlying, output_size, input_size, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_nearest3d(self: Tensor, output_size: Array[Long], scales_d: Option[Double], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest3d(self.underlying, output_size, scales_d.asJavaDouble, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def upsample_nearest3d_backward(grad_output: Tensor, output_size: Array[Long], input_size: Array[Long], scales_d: Option[Double], scales_h: Option[Double], scales_w: Option[Double])(implicit rm: ReferenceManager): Tensor = Tensor(swig.upsample_nearest3d_backward(grad_output.underlying, output_size, input_size, scales_d.asJavaDouble, scales_h.asJavaDouble, scales_w.asJavaDouble))
  def sigmoid_backward(grad_output: Tensor, output: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.sigmoid_backward(grad_output.underlying, output.underlying))
  def logit_backward(grad_output: Tensor, self: Tensor, eps: Option[Double] = None)(implicit rm: ReferenceManager): Tensor = Tensor(swig.logit_backward(grad_output.underlying, self.underlying, eps.asJavaDouble))
  def tanh_backward(grad_output: Tensor, output: Tensor)(implicit rm: ReferenceManager): Tensor = Tensor(swig.tanh_backward(grad_output.underlying, output.underlying))
  def slow_conv_transpose2d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0), output_padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1))(implicit rm: ReferenceManager): Tensor = Tensor(swig.slow_conv_transpose2d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, output_padding, dilation))
  def slow_conv_transpose2d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], output_padding: Array[Long], dilation: Array[Long], columns: Tensor, ones: Tensor, output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv_transpose2d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, output_padding, dilation, columns.underlying, ones.underlying, output_mask))
  def slow_conv_transpose3d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0), output_padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1))(implicit rm: ReferenceManager): Tensor = Tensor(swig.slow_conv_transpose3d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, output_padding, dilation))
  def slow_conv_transpose3d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], output_padding: Array[Long], dilation: Array[Long], finput: Tensor, fgrad_input: Tensor, output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv_transpose3d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, output_padding, dilation, finput.underlying, fgrad_input.underlying, output_mask))
  def thnn_conv2d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0))(implicit rm: ReferenceManager): Tensor = Tensor(swig.thnn_conv2d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding))
  def _conv_depthwise2d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor], stride: Array[Long], padding: Array[Long], dilation: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig._conv_depthwise2d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, dilation))
  def _conv_depthwise2d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor) = wrapTensorTuple2(swig._conv_depthwise2d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, dilation, output_mask))
  def conv_depthwise3d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor], stride: Array[Long], padding: Array[Long], dilation: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.conv_depthwise3d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, dilation))
  def conv_depthwise3d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.conv_depthwise3d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, dilation, output_mask))
  def slow_conv3d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0))(implicit rm: ReferenceManager): Tensor = Tensor(swig.slow_conv3d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding))
  def slow_conv3d_forward(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor], stride: Array[Long], padding: Array[Long])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv3d_forward(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding))
  def slow_conv3d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], finput: Tensor, fgrad_input: Tensor, output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv3d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, finput.underlying, fgrad_input.underlying, output_mask))
  def slow_conv_dilated2d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1))(implicit rm: ReferenceManager): Tensor = Tensor(swig.slow_conv_dilated2d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, dilation))
  def slow_conv_dilated2d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv_dilated2d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, dilation, output_mask))
  def slow_conv_dilated3d(self: Tensor, weight: Tensor, kernel_size: Array[Long], bias: Option[Tensor] = None, stride: Array[Long] = Array(1), padding: Array[Long] = Array(0), dilation: Array[Long] = Array(1))(implicit rm: ReferenceManager): Tensor = Tensor(swig.slow_conv_dilated3d(self.underlying, weight.underlying, kernel_size, bias.map(_.underlying), stride, padding, dilation))
  def slow_conv_dilated3d_backward(grad_output: Tensor, self: Tensor, weight: Tensor, kernel_size: Array[Long], stride: Array[Long], padding: Array[Long], dilation: Array[Long], output_mask: Array[Boolean])(implicit rm: ReferenceManager): (Tensor, Tensor, Tensor) = wrapTensorTuple3(swig.slow_conv_dilated3d_backward(grad_output.underlying, self.underlying, weight.underlying, kernel_size, stride, padding, dilation, output_mask))
  def col2im(self: Tensor, output_size: Array[Long], kernel_size: Array[Long], dilation: Array[Long], padding: Array[Long], stride: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.col2im(self.underlying, output_size, kernel_size, dilation, padding, stride))
  def col2im_backward(grad_output: Tensor, kernel_size: Array[Long], dilation: Array[Long], padding: Array[Long], stride: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.col2im_backward(grad_output.underlying, kernel_size, dilation, padding, stride))
  def im2col(self: Tensor, kernel_size: Array[Long], dilation: Array[Long], padding: Array[Long], stride: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.im2col(self.underlying, kernel_size, dilation, padding, stride))
  def im2col_backward(grad_output: Tensor, input_size: Array[Long], kernel_size: Array[Long], dilation: Array[Long], padding: Array[Long], stride: Array[Long])(implicit rm: ReferenceManager): Tensor = Tensor(swig.im2col_backward(grad_output.underlying, input_size, kernel_size, dilation, padding, stride))
  def _test_optional_intlist(values: Tensor, addends: Option[Array[Long]])(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_optional_intlist(values.underlying, addends))
  def _test_optional_filled_intlist(values: Tensor, addends: Option[Array[Long]])(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_optional_filled_intlist(values.underlying, addends))
  def _test_optional_floatlist(values: Tensor, addends: Option[Array[Double]])(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_optional_floatlist(values.underlying, addends))
  def _test_string_default(dummy: Tensor, a: String = "\"'\\", b: String = "\"'\\")(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_string_default(dummy.underlying, a, b))
  def _test_ambiguous_defaults(dummy: Tensor, a: Long = 1, b: Long = 1)(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_ambiguous_defaults(dummy.underlying, a, b))
  def _test_ambiguous_defaults(dummy: Tensor, a: Long, b: String)(implicit rm: ReferenceManager): Tensor = Tensor(swig._test_ambiguous_defaults(dummy.underlying, a, b))
  def pad_sequence(sequences: Array[Tensor], batch_first: Boolean = false, padding_value: Double = 0.0)(implicit rm: ReferenceManager): Tensor = Tensor(swig.pad_sequence(sequences.map(_.underlyingChecked), batch_first, padding_value))
  def flatten_dense_tensors(tensors: Array[Tensor])(implicit rm: ReferenceManager): Tensor = Tensor(swig.flatten_dense_tensors(tensors.map(_.underlyingChecked)))
  def unflatten_dense_tensors(flat: Tensor, tensors: Array[Tensor])(implicit rm: ReferenceManager): Array[Tensor] = tensorVectorToArray(swig.unflatten_dense_tensors(flat.underlying, tensors.map(_.underlyingChecked)))}
